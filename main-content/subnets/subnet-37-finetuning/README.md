---
description: Bittensor can train the best models
---

# Subnet 37 - Finetuning

Subnet 37 ambition is to bring the entire AI development pipeline into Bittensor, and we see fine-tuning as a critical step in the path to reaching this goal. We believe that this will strongly support Bittensor’s founding vision which is that AI can be built in an economical, safe and decentralized way.

Fine-tuning is costly, time consuming and highly limited by expertise. It requires hundreds of GPU hours, typically requiring SOTA hardware but, perhaps most importantly, it requires expert engineers with the know-how to do it well, who are hard to find.&#x20;

We address these challenges by creating a subnet which outsources the procurement process of the necessary computational resources and incentivises the best AI developers in the world to monetise their skills and experience by competing to produce the best models in collaboration with subnet 9 Pre-training.

<figure><img src="../../.gitbook/assets/Screenshot 2025-03-05 at 18.13.57.png" alt=""><figcaption></figcaption></figure>

This subnet is a general-purpose platform for running multiple fine-tuning competitions in parallel. Our vision is to build an open-sourced catalog of models, each optimised for specialised tasks such as chatbots, math-solvers, programming assistants, and recommendation bots. Models in the catalog are already available on Hugging Face for download and will soon be powering apps (e.g. Macrocosmos, opentensor.ai).

Furthermore, we aim to integrate the models we create into subnet 1 as base models for future agentic assistants. We see subnets 37 and 1 evolving together to produce ever-improving AI assistants. This will provide the additional benefit of real user feedback through subnet 1’s chat application, which will be used to continuously refine the models and their capabilities.

For more details about the subnet 37 R\&D work have a look our substack articles:

* [Fine-tuning, finely tuned: How SN37 is delivering SOTA fine-tuning on Bittensor](https://macrocosmosai.substack.com/p/fine-tuning-finely-tuned-how-sn37)
* [Fine-tuning, harmonized: Taoverse and Macrocosmos team up on SN37](https://macrocosmosai.substack.com/p/fine-tuning-harmonized-macrocosmos)

Related resources

* [Website](https://www.macrocosmos.ai/sn37)
* [Dashboard](https://www.macrocosmos.ai/sn37/dashboard)
* [GitHub](https://github.com/macrocosm-os/finetuning)
* [Substack](https://macrocosmosai.substack.com/t/ai-fine-tuning)
* [Bittensor Discord](https://discord.com/channels/799672011265015819/1234881153832321024)
* [Macrocosmos Discord](https://discord.com/channels/1238450997848707082)
* [Cosmonauts - Macrocosmos Telegram](https://t.me/macrocosmosai)
* [Macrocosmos X](https://x.com/MacrocosmosAI)

\
