---
description: Bittensor can build the best models
---

# Subnet 9 - Pre-training

Subnet 9 is the premier training subnet within the Bittensor ecosystem. It incentivises miners to optimise model training within given constraints such as model size and training dataset, providing intelligence ready for fine-tuning towards specific use cases.

<figure><img src="../../.gitbook/assets/Train overview.png" alt=""><figcaption></figcaption></figure>

Pre-training is expensive: the required computing power [doubles every 10 months](https://arxiv.org/pdf/2202.05924). [Research from Epoch AI](https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems#why-study-dollar-training-costs) estimates the dollar cost of LLM development is growing by 0.49 orders of magnitude a year. As the cost of model training increases, so does the risk that pre-training and fine-tuning at scale will only be accessible for the very largest companies.

By rewarding miners for sharing the best pre-training models, subnet 9’s design ensures a continuously improving baseline of intelligence for Bittensor. Marginal improvements can have significant downstream benefits. Over time, our aim is to build a library of open-source models at different sizes, modalities and architectures, which can then be fine-tuned across different subnets or teams outside the Bittensor ecosystem.

Our vision is to create an AI flywheel with subnet 9 at its center, creating a Bittensor-trained, fine-tuned, specialised and deployed inference subnet providing foundational models on demand with different datasets and architectures. New models will be powering agentic tools and other Apps in and out of the Bittensor environment.

The latest experiments not only proved that dataset mixing is viable on Bittensor, but SN9 also demonstrates how it can stimulate competitions, encourage innovation, and excite the community. What started out as a mere experiment into different pre-training architectures, has evolved into a fundamentally stronger subnet.

We’re now confident that, beyond volume and quality of public data, SN9 has no limits. However, most proprietary models don’t reveal their exact datasets, closing the door on open-source alternatives. Therefore, if we want to compete by continuously producing SOTA results, we must expand our repertoire of open-source and accessible datasets, and scale upwards so our models can be well-fed with information. Training with synthetic data is also becoming a mainstream approach in machine learning, and so this’ll be a part of our future plans, too.

By proving that the very best models can be built on Bittensor, we can elevate the entire protocol and push SN9 to the top of the LLM-training ecosystem. The success of dataset mixing is a step towards making that vision a reality.&#x20;

[Subnet 9 Whitepaper](https://www.macrocosmos.ai/research/pretraining_whitepaper.pdf) provides a detailed view to the pre-training.

For more details about the subnet 9 R\&D work have a look our substack articles:

* [Rebalanced, refined, and SOTA: Our latest dataset mixing results in SN9](https://macrocosmosai.substack.com/p/rebalancing-and-refining-our-dataset)
* [SN9: Dataset mixing delivers SOTA results](https://macrocosmosai.substack.com/p/sn9-dataset-mixing-delivers-sota)
* [Distributed learning for decentralized LLMs: preliminary results](https://macrocosmosai.substack.com/p/distributed-learning-for-decentralized)
* [SN9’s smarter dataset mixing: pushing the limits of our pre-training subnet (part 1)](https://macrocosmosai.substack.com/p/sn9s-smarter-dataset-mixing-pushing)
* [Monsters, vampires, and X-rays: subnet 9’s Halloween deep dive](https://macrocosmosai.substack.com/p/monsters-vampires-and-x-rays-subnet)

Other related resources

* [Website](https://www.macrocosmos.ai/sn9)
* [Dashboard](https://www.macrocosmos.ai/sn9/dashboard)
* [GitHub](https://github.com/macrocosm-os/pretraining)
* [Substack](https://macrocosmosai.substack.com/t/pre-training)
* [Bittensor Discord](https://discord.com/channels/799672011265015819/1162768567821930597)
* [Macrocosmos Discord](https://discord.com/channels/1238450997848707082)
* [Cosmonauts - Macrocosmos Telegram](https://t.me/macrocosmosai)
* [Macrocosmos X](https://x.com/MacrocosmosAI)
