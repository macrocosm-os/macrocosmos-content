---
description: Bittensor can build the best models
---

# Subnet 9 - Pre-training

Subnet 9 is Bittensor's premier training subnet. It incentivises miners to optimise model training within given constraints such as model size and training dataset, providing intelligence that's ready for fine-tuning towards specific use cases.

<figure><img src="../../.gitbook/assets/Train overview.png" alt=""><figcaption></figcaption></figure>

**Pre-training is expensive -** the required computing power for pre-training [doubles every 10 months](https://arxiv.org/pdf/2202.05924). Research from [Epoch AI](https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems#why-study-dollar-training-costs) estimates the dollar cost of LLM development is growing by 0.49 orders of magnitude a year. As this increases, so does the risk that pre-training and fine-tuning at scale will only be accessible for the very largest companies.

By rewarding miners for sharing the best pre-training models, subnet 9’s design ensures a continuously improving baseline of intelligence on Bittensor. Marginal improvements can have significant downstream benefits. Over time, we aim to build a library of open-source models at different sizes, modalities and architectures, which can then be fine-tuned across subnets or even teams outside the Bittensor ecosystem.

Our vision is to create an AI flywheel with subnet 9 at its center, building a Bittensor-trained, fine-tuned, specialized and deployed inference subnet providing foundational models on-demand with different datasets and architectures. New models will be powering agentic tools and other Apps in and out of the Bittensor environment.

Our latest experiments not only proved that dataset mixing is viable on Bittensor, but SN9 also shows how it can stimulate competitions, encourage innovation, and excite the community. What started out as a mere experiment into different pre-training architectures has evolved into a fundamentally stronger subnet.

We’re confident that, beyond volume and quality of public data, SN9 has no limits. However, most proprietary models don’t reveal their exact datasets, closing the door on open-source alternatives. Therefore, if we want to compete by continuously increasing SOTA results, we must expand our repertoire of open-source and accessible datasets, and scale upwards so our models can be well-fed with information. Training with synthetic data is also becoming a mainstream approach in machine learning, so this’ll be a part of our plans, too.

We can elevate the entire protocol and push SN9 to the top of the LLM-training ecosystem by highlighting Bittensor's capacity to build top-tier models. The success of dataset mixing is a step towards making that vision a reality.&#x20;

The [subnet 9 Whitepaper](https://www.macrocosmos.ai/research/pretraining_whitepaper.pdf) provides a detailed view of our pre-training efforts.

For more details about subnet 9's R\&D work, take a look our Substack articles:

* [Rebalanced, refined, and SOTA: Our latest dataset mixing results in SN9](https://macrocosmosai.substack.com/p/rebalancing-and-refining-our-dataset)
* [SN9: Dataset mixing delivers SOTA results](https://macrocosmosai.substack.com/p/sn9-dataset-mixing-delivers-sota)
* [Distributed learning for decentralized LLMs: preliminary results](https://macrocosmosai.substack.com/p/distributed-learning-for-decentralized)
* [SN9’s smarter dataset mixing: pushing the limits of our pre-training subnet (part 1)](https://macrocosmosai.substack.com/p/sn9s-smarter-dataset-mixing-pushing)
* [Monsters, vampires, and X-rays: subnet 9’s Halloween deep dive](https://macrocosmosai.substack.com/p/monsters-vampires-and-x-rays-subnet)

Other related resources

* [Website](https://www.macrocosmos.ai/sn9)
* [Dashboard](https://www.macrocosmos.ai/sn9/dashboard)
* [GitHub](https://github.com/macrocosm-os/pretraining)
* [Substack](https://macrocosmosai.substack.com/t/pre-training)
* [Bittensor Discord](https://discord.com/channels/799672011265015819/1162768567821930597)
* [Macrocosmos Discord](https://discord.com/channels/1238450997848707082)
* [Cosmonauts - Macrocosmos Telegram](https://t.me/macrocosmosai)
* [Macrocosmos X](https://x.com/MacrocosmosAI)
